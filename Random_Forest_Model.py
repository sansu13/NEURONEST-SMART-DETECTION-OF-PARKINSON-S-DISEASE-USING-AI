# -*- coding: utf-8 -*-
"""Final_Random_Forest.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1WTItdTwMPTXBiY2q-GIO3IteXTOsTcBQ
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, roc_curve, auc
from sklearn.ensemble import RandomForestClassifier
import joblib
from google.colab import files
import time
# Set seeds for reproducibility
np.random.seed(42)

print("Please upload your dataset CSV file...")
uploaded = files.upload()
filename = list(uploaded.keys())[0]
print(f"File uploaded: {filename}")

# Load the dataset
df = pd.read_csv(filename)
print("Dataset loaded successfully!")

# Extract features and target
print("\nPreparing data...")
if 'name' in df.columns:
    names = df['name'].values  # Save names for reference
    X = df.drop(['name', 'status'], axis=1)
else:
    X = df.drop(['status'], axis=1)
y = df['status'].astype(int)
df_numeric = df.drop(columns=['name', 'status'], errors='ignore') # Use errors='ignore' in case 'name' or 'status' is missing

plt.figure(figsize=(12, 10))
# Calculate correlation on the DataFrame with only numeric columns
sns.heatmap(df_numeric.corr(), annot=False, cmap='coolwarm')
plt.title('Feature Correlation Heatmap')
plt.show()

# Split data into training, validation, and testing sets with stratification
X_train, X_temp, y_train, y_temp = train_test_split(
    X, y, test_size=0.3, random_state=42, stratify=y
)
X_val, X_test, y_val, y_test = train_test_split(
    X_temp, y_temp, test_size=0.5, random_state=42, stratify=y_temp
)
print(f"Training set: {X_train.shape[0]} samples")
print(f"Validation set: {X_val.shape[0]} samples")
print(f"Test set: {X_test.shape[0]} samples")

# Scale features (fit only on training data)
print("\nScaling features...")
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_val_scaled = scaler.transform(X_val)
X_test_scaled = scaler.transform(X_test)

# Save the feature names and scaler
feature_names = list(X.columns)
with open('feature_names.txt', 'w') as f:
    f.write('\n'.join(feature_names))
print("\nSaving scaler...")
joblib.dump(scaler, 'scaler.pkl')

print("\n=========== RANDOM FOREST MODEL TRAINING ===========")

# CONTROL WITH A FLAG (AVOID input())
USE_GRID_SEARCH = True  # <<== Change this to True if you want to grid search

if USE_GRID_SEARCH:
    rf_params = {
        'n_estimators': [100, 200],
        'max_depth': [None, 10],
        'min_samples_split': [2, 5],
    }

    print("\nPerforming grid search for Random Forest (with smaller parameter grid)...")
    rf_grid = GridSearchCV(
        RandomForestClassifier(random_state=42),
        rf_params,
        cv=3,
        scoring='accuracy',
        n_jobs=1,  # Safer in Colab, prevents crashes
        verbose=1
    )

    start_time = time.time()
    rf_grid.fit(X_train_scaled, y_train)
    end_time = time.time()

    print(f"Grid search completed in {end_time - start_time:.2f} seconds")
    best_rf = rf_grid.best_estimator_
    print(f"\nBest Random Forest parameters:\n{rf_grid.best_params_}")
else:
    print("\nSkipping grid search, using default Random Forest parameters...")
    best_rf = RandomForestClassifier(
        n_estimators=100,
        max_depth=10,
        min_samples_split=2,
        random_state=42
    )
    best_rf.fit(X_train_scaled, y_train)
    print("Default Random Forest model trained successfully")

# Validate the model
rf_val_pred = best_rf.predict(X_val_scaled)
rf_val_accuracy = accuracy_score(y_val, rf_val_pred)
print(f"\nRandom Forest validation accuracy: {rf_val_accuracy:.4f}")

# Print classification report for validation set
print("\nValidation set classification report:")
print(classification_report(y_val, rf_val_pred))

# Create confusion matrix for validation set
plt.figure(figsize=(8, 6))
cm = confusion_matrix(y_val, rf_val_pred)
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')
plt.title('Random Forest - Validation Confusion Matrix')
plt.xlabel('Predicted Label')
plt.ylabel('True Label')
plt.show()

# Calculate ROC curve and AUC for validation set
rf_val_proba = best_rf.predict_proba(X_val_scaled)[:, 1]
fpr, tpr, _ = roc_curve(y_val, rf_val_proba)
roc_auc = auc(fpr, tpr)

plt.figure(figsize=(8, 6))
plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (area = {roc_auc:.2f})')
plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Random Forest - ROC Curve (Validation Set)')
plt.legend(loc="lower right")
plt.show()

# Evaluate on test set
rf_test_pred = best_rf.predict(X_test_scaled)
rf_test_accuracy = accuracy_score(y_test, rf_test_pred)
print(f"\nRandom Forest test accuracy: {rf_test_accuracy:.4f}")

# Print classification report for test set
print("\nTest set classification report:")
print(classification_report(y_test, rf_test_pred))

# Create confusion matrix for test set
plt.figure(figsize=(8, 6))
cm = confusion_matrix(y_test, rf_test_pred)
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')
plt.title('Random Forest - Test Confusion Matrix')
plt.xlabel('Predicted Label')
plt.ylabel('True Label')
plt.show()

# Calculate ROC curve and AUC for test set
rf_test_proba = best_rf.predict_proba(X_test_scaled)[:, 1]
fpr, tpr, _ = roc_curve(y_test, rf_test_proba)
roc_auc = auc(fpr, tpr)

plt.figure(figsize=(8, 6))
plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (area = {roc_auc:.2f})')
plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Random Forest - ROC Curve (Test Set)')
plt.legend(loc="lower right")
plt.show()

# Feature importance
plt.figure(figsize=(12, 8))
importances = best_rf.feature_importances_
indices = np.argsort(importances)[::-1]
plt.title('Random Forest Feature Importance')
plt.bar(range(X_train.shape[1]), importances[indices], align='center')
plt.xticks(range(X_train.shape[1]), [feature_names[i] for i in indices], rotation=90)
plt.tight_layout()
plt.show()

# Save the model
print("\nSaving the Random Forest model...")
joblib.dump(best_rf, 'random_forest_model.pkl')

# Part 2: Test with new data
print("\n\n========= TESTING WITH NEW DATA =========\n")
print("Please upload your Test CSV file...")
uploaded = files.upload()
filename = list(uploaded.keys())[0]
print(f"Test file uploaded: {filename}")

# Load the test dataset
test_df = pd.read_csv(filename)
print("Test dataset loaded successfully!")

# Extract names before dropping any columns
if 'name' in test_df.columns:
    test_names = test_df['name'].values
else:
    test_names = [f"Patient_{i+1}" for i in range(len(test_df))]

# Drop unnecessary columns if present
test_features = test_df.copy()
if 'name' in test_features.columns:
    test_features.drop(columns=['name'], inplace=True)
if 'status' in test_features.columns:
    test_features.drop(columns=['status'], inplace=True)

# Check for feature consistency
print("\nChecking feature consistency...")
missing_cols = set(feature_names) - set(test_features.columns)
extra_cols = set(test_features.columns) - set(feature_names)
if missing_cols:
    print(f"Warning: Test data is missing columns that were in training data: {missing_cols}")
    for col in missing_cols:
        test_features[col] = 0
if extra_cols:
    print(f"Warning: Test data has extra columns that were not in training data: {extra_cols}")
    test_features = test_features.drop(columns=list(extra_cols))

# Ensure columns are in the same order as during training
test_features = test_features[feature_names]

# Scale the test features using the same scaler
test_features_scaled = scaler.transform(test_features)

# Make predictions with Random Forest
print("\nMaking predictions with Random Forest model...")
test_predictions = best_rf.predict(test_features_scaled)
test_probabilities = best_rf.predict_proba(test_features_scaled)[:, 1]

# Create results dataframe
results_df = pd.DataFrame({
    'Name': test_names,
    'Predicted_Status': test_predictions,
    'Disease_Probability': test_probabilities
})

# Display results in requested format
print("\nPrediction Results:")
for i in range(len(test_names)):
    prob = test_probabilities[i]
    pred = test_predictions[i]
    print(f"{test_names[i]}: {prob:.4f}")

# Save results to CSV
results_df.to_csv('random_forest_predictions.csv', index=False)
print("\nResults saved to 'random_forest_predictions.csv'")

# If ground truth is available in test data, evaluate performance
if 'status' in test_df.columns:
    y_true = test_df['status'].astype(int)
    print("\nEvaluation on the new test data:")
    print(f"Accuracy: {accuracy_score(y_true, test_predictions):.4f}")
    print("\nClassification Report:")
    print(classification_report(y_true, test_predictions))

    # Confusion matrix
    plt.figure(figsize=(8, 6))
    cm = confusion_matrix(y_true, test_predictions)
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')
    plt.title('Random Forest - New Test Data Confusion Matrix')
    plt.xlabel('Predicted Label')
    plt.ylabel('True Label')
    plt.show()
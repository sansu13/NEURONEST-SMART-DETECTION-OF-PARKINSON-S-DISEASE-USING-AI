# -*- coding: utf-8 -*-
"""decision_tree_final.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1B8lYNLPhtYi9sRtrgT6TGGjf8s22Til6
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score
from sklearn.tree import DecisionTreeRegressor
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import (
    mean_squared_error, mean_absolute_error, r2_score,
    accuracy_score, precision_score, recall_score, f1_score,
    confusion_matrix, classification_report, roc_auc_score,
    roc_curve, precision_recall_curve
)
import joblib
from scipy.stats import pearsonr
import warnings
warnings.filterwarnings('ignore')

# Upload dataset
print("Please upload your dataset CSV file...")
uploaded = files.upload()
filename = list(uploaded.keys())[0]
print(f"File uploaded: {filename}")

# Load your dataset
print("Loading Parkinson's disease dataset...")
# Replace 'your_dataset.csv' with your actual file path
print(f"Dataset shape: {df.shape}")
print("\nFirst few rows:")
print(df.head())

# Check the distribution of the target variable
print(f"\nTarget variable 'status' statistics:")
print(df['status'].describe())
print(f"\nUnique values in status: {sorted(df['status'].unique())}")

# Data exploration
plt.figure(figsize=(15, 5))
# Plot 1: Distribution of target variable
plt.subplot(1, 3, 1)
plt.hist(df['status'], bins=30, alpha=0.7, color='skyblue', edgecolor='black')
plt.title('Distribution of Status (Probability)')
plt.xlabel('Status Value')
plt.ylabel('Frequency')

# Plot 2: Count of binary classification (0 vs non-zero)
plt.subplot(1, 3, 2)
binary_status = (df['status'] > 0).astype(int)
binary_counts = binary_status.value_counts()
plt.bar(['Healthy (0)', 'Disease (>0)'], binary_counts.values, color=['lightgreen', 'lightcoral'])
plt.title('Binary Classification Distribution')
plt.ylabel('Count')
# Plot 3: Boxplot of features (excluding name and status)
plt.subplot(1, 3, 3)
feature_cols = [col for col in df.columns if col not in ['name', 'status']]
sample_features = feature_cols[:5]  # Show first 5 features to avoid clutter
df[sample_features].boxplot()
plt.title('Sample Feature Distributions')
plt.xticks(rotation=45)
plt.tight_layout()
plt.show()

# Prepare the data
print("\nPreparing data...")
X = df.drop(['name', 'status'], axis=1) if 'name' in df.columns else df.drop(['status'], axis=1)
y = df['status']
print(f"Features shape: {X.shape}")
print(f"Target shape: {y.shape}")
print(f"Feature columns: {list(X.columns)}")
# Check for missing values
print(f"\nMissing values in features: {X.isnull().sum().sum()}")
print(f"Missing values in target: {y.isnull().sum()}")
# Handle missing values if any
if X.isnull().sum().sum() > 0:
    X = X.fillna(X.median())
    print("Missing values filled with median")

# Split the data
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42, stratify=(y > 0).astype(int)
)

# Scale the features
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

print(f"\nTraining set shape: {X_train_scaled.shape}")
print(f"Test set shape: {X_test_scaled.shape}")

# Train Decision Tree Regressor
print("\n" + "="*50)
print("TRAINING DECISION TREE REGRESSOR")
print("="*50)
# Hyperparameter tuning for Decision Tree
dt_params = {
    'max_depth': [3, 5, 7, 10, None],
    'min_samples_split': [2, 5, 10, 20],
    'min_samples_leaf': [1, 2, 4, 8],
    'max_features': ['sqrt', 'log2', None]
}

dt_grid = GridSearchCV(
    DecisionTreeRegressor(random_state=42),
    dt_params,
    cv=5,
    scoring='neg_mean_squared_error',
    n_jobs=-1,
    verbose=1
)
dt_grid.fit(X_train_scaled, y_train)
best_dt = dt_grid.best_estimator_

print(f"Best Decision Tree parameters: {dt_grid.best_params_}")
print(f"Best cross-validation score: {-dt_grid.best_score_:.4f}")

# Train Random Forest for comparison (simplified for faster execution)
print("\nTraining Random Forest for comparison...")
rf_params = {
    'n_estimators': [50, 100],
    'max_depth': [5, 10],
    'min_samples_split': [5, 10]
}

rf_grid = GridSearchCV(
    RandomForestRegressor(random_state=42, n_jobs=1),  # Reduced n_jobs for stability
    rf_params,
    cv=3,  # Reduced CV folds for speed
    scoring='neg_mean_squared_error',
    n_jobs=1,  # Set to 1 to avoid multiprocessing issues
    verbose=0  # Reduced verbosity
)

print("Fitting Random Forest (this may take a moment)...")
rf_grid.fit(X_train_scaled, y_train)
best_rf = rf_grid.best_estimator_
print(f"Random Forest training completed!")
print(f"Best RF parameters: {rf_grid.best_params_}")

# Make predictions
dt_pred = best_dt.predict(X_test_scaled)
rf_pred = best_rf.predict(X_test_scaled)

# Clip predictions to valid probability range [0, 1]
dt_pred = np.clip(dt_pred, 0, 1)
rf_pred = np.clip(rf_pred, 0, 1)

# Regression Metrics
print("\n" + "="*50)
print("REGRESSION METRICS")
print("="*50)

models = {'Decision Tree': dt_pred, 'Random Forest': rf_pred}

for name, pred in models.items():
    mse = mean_squared_error(y_test, pred)
    mae = mean_absolute_error(y_test, pred)
    r2 = r2_score(y_test, pred)
    corr, _ = pearsonr(y_test, pred)

    print(f"\n{name} Results:")
    print(f"  MSE: {mse:.4f}")
    print(f"  RMSE: {np.sqrt(mse):.4f}")
    print(f"  MAE: {mae:.4f}")
    print(f"  RÂ² Score: {r2:.4f}")
    print(f"  Correlation: {corr:.4f}")

# Classification Metrics (using threshold of 0.5)
print("\n" + "="*50)
print("CLASSIFICATION METRICS (Threshold = 0.5)")
print("="*50)

threshold = 0.5
y_test_binary = (y_test >= threshold).astype(int)

for name, pred in models.items():
    pred_binary = (pred >= threshold).astype(int)

    acc = accuracy_score(y_test_binary, pred_binary)
    prec = precision_score(y_test_binary, pred_binary, zero_division=0)
    rec = recall_score(y_test_binary, pred_binary, zero_division=0)
    f1 = f1_score(y_test_binary, pred_binary, zero_division=0)

    print(f"\n{name} Classification Results:")
    print(f"  Accuracy: {acc:.4f}")
    print(f"  Precision: {prec:.4f}")
    print(f"  Recall: {rec:.4f}")
    print(f"  F1-Score: {f1:.4f}")

    if len(np.unique(y_test_binary)) > 1:
        auc = roc_auc_score(y_test_binary, pred)
        print(f"  ROC AUC: {auc:.4f}")

# Visualization
plt.figure(figsize=(20, 12))

# 1. Actual vs Predicted scatter plots
plt.subplot(3, 4, 1)
plt.scatter(y_test, dt_pred, alpha=0.6, color='blue')
plt.plot([0, 1], [0, 1], 'r--', lw=2)
plt.xlabel('Actual Values')
plt.ylabel('Predicted Values')
plt.title('Decision Tree: Actual vs Predicted')

plt.subplot(3, 4, 2)
plt.scatter(y_test, rf_pred, alpha=0.6, color='green')
plt.plot([0, 1], [0, 1], 'r--', lw=2)
plt.xlabel('Actual Values')
plt.ylabel('Predicted Values')
plt.title('Random Forest: Actual vs Predicted')

# 2. Residual plots
plt.subplot(3, 4, 3)
residuals_dt = y_test - dt_pred
plt.scatter(dt_pred, residuals_dt, alpha=0.6, color='blue')
plt.axhline(y=0, color='r', linestyle='--')
plt.xlabel('Predicted Values')
plt.ylabel('Residuals')
plt.title('Decision Tree: Residuals')

plt.subplot(3, 4, 4)
residuals_rf = y_test - rf_pred
plt.scatter(rf_pred, residuals_rf, alpha=0.6, color='green')
plt.axhline(y=0, color='r', linestyle='--')
plt.xlabel('Predicted Values')
plt.ylabel('Residuals')
plt.title('Random Forest: Residuals')

# 3. Feature importance
plt.subplot(3, 4, 5)
dt_importance = pd.Series(best_dt.feature_importances_, index=X.columns).sort_values(ascending=True)
dt_importance.tail(10).plot(kind='barh', color='skyblue')
plt.title('Decision Tree: Top 10 Feature Importance')
plt.xlabel('Importance')

plt.subplot(3, 4, 6)
rf_importance = pd.Series(best_rf.feature_importances_, index=X.columns).sort_values(ascending=True)
rf_importance.tail(10).plot(kind='barh', color='lightgreen')
plt.title('Random Forest: Top 10 Feature Importance')
plt.xlabel('Importance')

# 4. Distribution of predictions
plt.subplot(3, 4, 7)
plt.hist(dt_pred, bins=30, alpha=0.7, color='blue', label='Predicted', edgecolor='black')
plt.hist(y_test, bins=30, alpha=0.7, color='orange', label='Actual', edgecolor='black')
plt.xlabel('Probability')
plt.ylabel('Frequency')
plt.title('Decision Tree: Prediction Distribution')
plt.legend()

plt.subplot(3, 4, 8)
plt.hist(rf_pred, bins=30, alpha=0.7, color='green', label='Predicted', edgecolor='black')
plt.hist(y_test, bins=30, alpha=0.7, color='orange', label='Actual', edgecolor='black')
plt.xlabel('Probability')
plt.ylabel('Frequency')
plt.title('Random Forest: Prediction Distribution')
plt.legend()

# 5. Confusion matrices (for binary classification)
if len(np.unique(y_test_binary)) > 1:
    dt_pred_binary = (dt_pred >= threshold).astype(int)
    rf_pred_binary = (rf_pred >= threshold).astype(int)

    plt.subplot(3, 4, 9)
    cm_dt = confusion_matrix(y_test_binary, dt_pred_binary)
    sns.heatmap(cm_dt, annot=True, fmt='d', cmap='Blues', cbar=False)
    plt.title('Decision Tree: Confusion Matrix')
    plt.ylabel('Actual')
    plt.xlabel('Predicted')

    plt.subplot(3, 4, 10)
    cm_rf = confusion_matrix(y_test_binary, rf_pred_binary)
    sns.heatmap(cm_rf, annot=True, fmt='d', cmap='Greens', cbar=False)
    plt.title('Random Forest: Confusion Matrix')
    plt.ylabel('Actual')
    plt.xlabel('Predicted')

# 6. ROC Curves
if len(np.unique(y_test_binary)) > 1:
    plt.subplot(3, 4, 11)
    fpr_dt, tpr_dt, _ = roc_curve(y_test_binary, dt_pred)
    fpr_rf, tpr_rf, _ = roc_curve(y_test_binary, rf_pred)

    plt.plot(fpr_dt, tpr_dt, color='blue', label=f'Decision Tree (AUC = {roc_auc_score(y_test_binary, dt_pred):.3f})')
    plt.plot(fpr_rf, tpr_rf, color='green', label=f'Random Forest (AUC = {roc_auc_score(y_test_binary, rf_pred):.3f})')
    plt.plot([0, 1], [0, 1], color='red', linestyle='--')
    plt.xlabel('False Positive Rate')
    plt.ylabel('True Positive Rate')
    plt.title('ROC Curves')
    plt.legend()

# 7. Precision-Recall Curves
if len(np.unique(y_test_binary)) > 1:
    plt.subplot(3, 4, 12)
    precision_dt, recall_dt, _ = precision_recall_curve(y_test_binary, dt_pred)
    precision_rf, recall_rf, _ = precision_recall_curve(y_test_binary, rf_pred)

    plt.plot(recall_dt, precision_dt, color='blue', label='Decision Tree')
    plt.plot(recall_rf, precision_rf, color='green', label='Random Forest')
    plt.xlabel('Recall')
    plt.ylabel('Precision')
    plt.title('Precision-Recall Curves')
    plt.legend()

plt.tight_layout()
plt.show()

# Model Interpretation
print("\n" + "="*50)
print("MODEL INTERPRETATION")
print("="*50)

# Feature importance analysis
print("\nTop 10 Most Important Features (Decision Tree):")
dt_feat_imp = pd.DataFrame({
    'feature': X.columns,
    'importance': best_dt.feature_importances_
}).sort_values('importance', ascending=False)

for i, row in dt_feat_imp.head(10).iterrows():
    print(f"  {row['feature']}: {row['importance']:.4f}")

# Save the models and preprocessing objects
print("\n" + "="*50)
print("SAVING MODELS AND PREPROCESSORS")
print("="*50)

joblib.dump(best_dt, 'parkinson_decision_tree_model.pkl')
joblib.dump(best_rf, 'parkinson_random_forest_model.pkl')
joblib.dump(scaler, 'parkinson_scaler.pkl')

# Save feature names
with open('feature_names.txt', 'w') as f:
    f.write('\n'.join(X.columns))

print("Models and preprocessors saved successfully!")
print("Files saved:")
print("  - parkinson_decision_tree_model.pkl")
print("  - parkinson_random_forest_model.pkl")
print("  - parkinson_scaler.pkl")
print("  - feature_names.txt")
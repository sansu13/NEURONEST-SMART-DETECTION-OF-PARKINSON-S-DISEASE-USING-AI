# -*- coding: utf-8 -*-
"""Final_ANN.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1qRBTyGdSG42VjNCovvbYPDkrNamPDoKG
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, roc_curve, auc, roc_auc_score
import tensorflow as tf
from tensorflow.keras.models import Sequential, load_model
from tensorflow.keras.layers import Dense, Dropout, BatchNormalization
from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint
from tensorflow.keras.optimizers import Adam
import joblib
from google.colab import files

# Set seeds for reproducibility
np.random.seed(42)
tf.random.set_seed(42)

print("Please upload your dataset CSV file...")
uploaded = files.upload()
filename = list(uploaded.keys())[0]
print(f"File uploaded: {filename}")

# Load the dataset
df = pd.read_csv(filename)
print("Dataset loaded successfully!")

# Extract features and target
print("\nPreparing data...")
if 'name' in df.columns:
    names = df['name'].values  # Save names for reference
    X = df.drop(['name', 'status'], axis=1)
else:
    X = df.drop(['status'], axis=1)

y = df['status'].astype(int)

# Check class distribution (important!)
print("\nClass distribution:")
print(y.value_counts())
print(f"Class 0 (Healthy): {100 * y.value_counts()[0] / len(y):.2f}%")
print(f"Class 1 (Disease): {100 * y.value_counts()[1] / len(y):.2f}%")

# Split data into training, validation and testing sets with stratification
X_train, X_temp, y_train, y_temp = train_test_split(
    X, y, test_size=0.3, random_state=42, stratify=y
)
X_val, X_test, y_val, y_test = train_test_split(
    X_temp, y_temp, test_size=0.5, random_state=42, stratify=y_temp
)

print(f"Training set: {X_train.shape[0]} samples")
print(f"Validation set: {X_val.shape[0]} samples")
print(f"Test set: {X_test.shape[0]} samples")

# Scale features (fit only on training data)
print("\nScaling features...")
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_val_scaled = scaler.transform(X_val)
X_test_scaled = scaler.transform(X_test)

# Save the feature names and scaler
feature_names = list(X.columns)
with open('feature_names.txt', 'w') as f:
    f.write('\n'.join(feature_names))
print("\nSaving scaler...")
joblib.dump(scaler, 'scaler.pkl')

# Build improved ANN model
print("\nBuilding ANN model...")
input_dim = X_train_scaled.shape[1]

ann_model = Sequential([
    # Input layer with dropout to prevent overfitting
    Dense(64, activation='relu', input_shape=(input_dim,),
          kernel_initializer='he_normal', kernel_regularizer=tf.keras.regularizers.l2(0.001)),
    BatchNormalization(),
    Dropout(0.3),

    # Hidden layer
    Dense(32, activation='relu',
          kernel_initializer='he_normal', kernel_regularizer=tf.keras.regularizers.l2(0.001)),
    BatchNormalization(),
    Dropout(0.2),

    # Output layer - sigmoid for binary classification
    Dense(1, activation='sigmoid')
])

# Compile model with appropriate loss function for binary classification
ann_model.compile(
    optimizer=Adam(learning_rate=0.001),
    loss='binary_crossentropy',
    metrics=['accuracy', tf.keras.metrics.AUC()]
)

# Display model summary
ann_model.summary()

# Implement callbacks to prevent overfitting
early_stopping = EarlyStopping(
    monitor='val_loss',
    patience=20,
    restore_best_weights=True,
    verbose=1
)

model_checkpoint = ModelCheckpoint(
    'best_ann_model.h5',
    monitor='val_loss',
    save_best_only=True,
    verbose=1
)

# Train the model with validation data
print("\nTraining ANN model...")
history = ann_model.fit(
    X_train_scaled, y_train,
    epochs=150,
    batch_size=16,
    validation_data=(X_val_scaled, y_val),
    callbacks=[early_stopping, model_checkpoint],
    verbose=1
)

# Plot training history
plt.figure(figsize=(12, 5))

# Plot accuracy
plt.subplot(1, 2, 1)
plt.plot(history.history['accuracy'], label='Train Accuracy')
plt.plot(history.history['val_accuracy'], label='Validation Accuracy')
plt.title('Model Accuracy')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.legend()

# Plot loss
plt.subplot(1, 2, 2)
plt.plot(history.history['loss'], label='Train Loss')
plt.plot(history.history['val_loss'], label='Validation Loss')
plt.title('Model Loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend()

plt.tight_layout()
plt.savefig('training_history.png')
plt.show()

# Load the best model for evaluation
print("\nLoading best model...")
best_model = load_model('best_ann_model.h5')

# Evaluate model on test set
print("\nEvaluating on test set...")
test_loss, test_acc, test_auc = best_model.evaluate(X_test_scaled, y_test, verbose=0)
print(f"Test Accuracy: {test_acc:.4f}")
print(f"Test AUC: {test_auc:.4f}")
print(f"Test Loss: {test_loss:.4f}")

# Generate predictions
y_pred_proba = best_model.predict(X_test_scaled).flatten()
y_pred = (y_pred_proba >= 0.5).astype(int)

# Display classification report
print("\nClassification Report:")
print(classification_report(y_test, y_pred))

# Display confusion matrix
cm = confusion_matrix(y_test, y_pred)
plt.figure(figsize=(8, 6))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False)
plt.title('Test Set Confusion Matrix')
plt.xlabel('Predicted Label')
plt.ylabel('True Label')
plt.savefig('confusion_matrix.png')
plt.show()

# Plot ROC curve
fpr, tpr, _ = roc_curve(y_test, y_pred_proba)
roc_auc = auc(fpr, tpr)

plt.figure(figsize=(8, 6))
plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (area = {roc_auc:.2f})')
plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver Operating Characteristic (ROC)')
plt.legend(loc="lower right")
plt.savefig('roc_curve.png')
plt.show()

# Evaluate on validation set
print("\nEvaluating on validation set...")
val_pred_proba = best_model.predict(X_val_scaled).flatten()
val_pred = (val_pred_proba >= 0.5).astype(int)

# Classification report
print("\nValidation Classification Report:")
print(classification_report(y_val, val_pred))

# Confusion matrix
cm_val = confusion_matrix(y_val, val_pred)
plt.figure(figsize=(8, 6))
sns.heatmap(cm_val, annot=True, fmt='d', cmap='Purples', cbar=False)
plt.title('Validation Confusion Matrix')
plt.xlabel('Predicted Label')
plt.ylabel('True Label')
plt.savefig('val_confusion_matrix.png')
plt.show()

# ROC curve
fpr_val, tpr_val, _ = roc_curve(y_val, val_pred_proba)
roc_auc_val = auc(fpr_val, tpr_val)

plt.figure(figsize=(8, 6))
plt.plot(fpr_val, tpr_val, color='green', lw=2, label=f'Validation ROC (AUC = {roc_auc_val:.2f})')
plt.plot([0, 1], [0, 1], color='gray', lw=2, linestyle='--')
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Validation ROC Curve')
plt.legend(loc="lower right")
plt.savefig('val_roc_curve.png')
plt.show()

# Combined ROC plot
plt.figure(figsize=(8, 6))
plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'Test ROC (AUC = {roc_auc:.2f})')
plt.plot(fpr_val, tpr_val, color='green', lw=2, label=f'Validation ROC (AUC = {roc_auc_val:.2f})')
plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Combined ROC Curve')
plt.legend(loc="lower right")
plt.savefig('combined_roc_curve.png')
plt.show()

# Save the final model
print("\nSaving the final model...")
best_model.save('ann_model.h5')

print("\nModel training completed successfully!")

# Part 2: Test with new data
print("\n\n========= TESTING WITH NEW DATA =========\n")
print("Please upload your Test CSV file...")
uploaded = files.upload()
filename = list(uploaded.keys())[0]
print(f"Test file uploaded: {filename}")

# Load the test dataset
test_df = pd.read_csv(filename)
print("Test dataset loaded successfully!")

# Extract names before dropping any columns
if 'name' in test_df.columns:
    test_names = test_df['name'].values
else:
    test_names = [f"Patient_{i+1}" for i in range(len(test_df))]

# Drop unnecessary columns if present
test_features = test_df.copy()
if 'name' in test_features.columns:
    test_features.drop(columns=['name'], inplace=True)
if 'status' in test_features.columns:
    test_features.drop(columns=['status'], inplace=True)

# Check for feature consistency
print("\nChecking feature consistency...")
missing_cols = set(feature_names) - set(test_features.columns)
extra_cols = set(test_features.columns) - set(feature_names)

if missing_cols:
    print(f"Warning: Test data is missing columns that were in training data: {missing_cols}")
    # Add missing columns with zeros
    for col in missing_cols:
        test_features[col] = 0

if extra_cols:
    print(f"Warning: Test data has extra columns that were not in training data: {extra_cols}")
    # Remove extra columns
    test_features = test_features.drop(columns=list(extra_cols))

# Ensure columns are in the same order as during training
test_features = test_features[feature_names]

# Scale the test features using the same scaler
test_features_scaled = scaler.transform(test_features)

# Make predictions
print("\nGenerating predictions...")
test_proba = best_model.predict(test_features_scaled).flatten()
test_pred = (test_proba >= 0.5).astype(int)

# Show predictions with probabilities
print("\nPrediction Results:\n")
results = []
for name, prob, pred in zip(test_names, test_proba, test_pred):
    diagnosis = "Disease Present" if pred == 1 else "Healthy"
    confidence = prob if pred == 1 else 1 - prob
    results.append({
        'Patient': name,
        'Probability': prob,
        'Prediction': diagnosis,
        'Confidence': confidence
    })
    print(f"{name}: {prob:.4f} -> {diagnosis} (confidence: {confidence:.4f})")

# Create a DataFrame for results
results_df = pd.DataFrame(results)
results_df.to_csv('prediction_results.csv', index=False)
print("\nResults saved to 'prediction_results.csv'")

# Visualize distribution of prediction probabilities
plt.figure(figsize=(10, 6))
sns.histplot(test_proba, bins=20, kde=True)
plt.axvline(x=0.5, color='r', linestyle='--', label='Decision Threshold (0.5)')
plt.title('Distribution of Prediction Probabilities')
plt.xlabel('Probability of Disease')
plt.ylabel('Count')
plt.legend()
plt.savefig('prediction_distribution.png')
plt.show()

print("\nAnalysis completed successfully!")
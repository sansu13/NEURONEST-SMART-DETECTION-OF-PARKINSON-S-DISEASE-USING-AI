# -*- coding: utf-8 -*-
"""Logistic_Regression.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1_QaS-c0G3q_rltutvWblvV8YAkn_k6ZO
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, roc_curve, auc
from sklearn.linear_model import LogisticRegression
import joblib
from google.colab import files
import time
from keras.models import Sequential
from keras.layers import Dense, Dropout
# Set seeds for reproducibility
np.random.seed(42)

print("Please upload your dataset CSV file...")
uploaded = files.upload()
filename = list(uploaded.keys())[0]
print(f"File uploaded: {filename}")

# Load the dataset
df = pd.read_csv(filename)
print("Dataset loaded successfully!")

# Extract features and target
print("\nPreparing data...")
if 'name' in df.columns:
    names = df['name'].values  # Save names for reference
    X = df.drop(['name', 'status'], axis=1)
else:
    X = df.drop(['status'], axis=1)
y = df['status'].astype(int)

plt.figure(figsize=(6, 4))
sns.countplot(x=y)
plt.title("Class Distribution")
plt.xlabel("Status")
plt.ylabel("Count")
plt.savefig('class_distribution.png')

# Split data into training, validation, and testing sets with stratification
X_train, X_temp, y_train, y_temp = train_test_split(
    X, y, test_size=0.3, random_state=42, stratify=y
)
X_val, X_test, y_val, y_test = train_test_split(
    X_temp, y_temp, test_size=0.5, random_state=42, stratify=y_temp
)
print(f"Training set: {X_train.shape[0]} samples")
print(f"Validation set: {X_val.shape[0]} samples")
print(f"Test set: {X_test.shape[0]} samples")

# Scale features (fit only on training data)
print("\nScaling features...")
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_val_scaled = scaler.transform(X_val)
X_test_scaled = scaler.transform(X_test)

# Save the feature names and scaler
feature_names = list(X.columns)
with open('feature_names.txt', 'w') as f:
    f.write('\n'.join(feature_names))
print("\nSaving scaler...")
joblib.dump(scaler, 'scaler.pkl')

# Logistic Regression Model Training
print("\n=========== LOGISTIC REGRESSION MODEL TRAINING ===========")

# Ask user if they want to do grid search or use default model
use_grid_search = input("\nDo you want to perform grid search for optimal parameters? (y/n): ").strip().lower()

if use_grid_search == 'y':
    # Define parameter grid for Logistic Regression
    log_reg_params = {
        'C': [0.01, 0.1, 1, 10, 100],
        'penalty': ['l1', 'l2'],
        'solver': ['liblinear'],
        'max_iter': [100, 200, 500]
    }

    print("\nPerforming grid search for Logistic Regression...")
    print("This might take a few minutes...")

    log_reg_grid = GridSearchCV(
        LogisticRegression(random_state=42),
        log_reg_params,
        cv=3,
        scoring='accuracy',
        n_jobs=2,
        verbose=1
    )

    start_time = time.time()
    log_reg_grid.fit(X_train_scaled, y_train)
    end_time = time.time()
    print(f"Grid search completed in {end_time - start_time:.2f} seconds")

    # Get the best model
    best_log_reg = log_reg_grid.best_estimator_
    print(f"\nBest Logistic Regression parameters:\n{log_reg_grid.best_params_}")

    results = pd.DataFrame(log_reg_grid.cv_results_)
    try:
        # Create a pivot table with unique (C, penalty) pairs
        pivot_table = results.pivot_table(
            index=['param_C', 'param_penalty'],
            columns='param_max_iter',
            values='mean_test_score'
        )

        plt.figure(figsize=(10, 8))
        sns.heatmap(pivot_table, annot=True, fmt=".3f", cmap='viridis')
        plt.title("Grid Search Accuracy Scores")
        plt.xlabel("Max Iterations")
        plt.ylabel("C + Penalty")
        plt.tight_layout()
        plt.show()
    except Exception as e:
        print(f"Could not generate heatmap: {e}")

coef_df = pd.DataFrame({
    'Feature': X_train.columns,
    'Coefficient': best_log_reg.coef_[0]
}).sort_values(by='Coefficient', key=abs, ascending=False)

plt.figure(figsize=(10, 6))
sns.barplot(data=coef_df, x='Coefficient', y='Feature')
plt.title('Logistic Regression Coefficients')
plt.tight_layout()
plt.show()

# Validate the model
log_reg_val_pred = best_log_reg.predict(X_val_scaled)
log_reg_val_accuracy = accuracy_score(y_val, log_reg_val_pred)
print(f"\nLogistic Regression validation accuracy: {log_reg_val_accuracy:.4f}")

# Print classification report for validation set
print("\nValidation set classification report:")
print(classification_report(y_val, log_reg_val_pred))

# Create confusion matrix for validation set
plt.figure(figsize=(8, 6))
cm = confusion_matrix(y_val, log_reg_val_pred)
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')
plt.title('Logistic Regression - Validation Confusion Matrix')
plt.xlabel('Predicted Label')
plt.ylabel('True Label')

# Calculate ROC curve and AUC for validation set
log_reg_val_proba = best_log_reg.predict_proba(X_val_scaled)[:, 1]
fpr, tpr, _ = roc_curve(y_val, log_reg_val_proba)
roc_auc = auc(fpr, tpr)

plt.figure(figsize=(8, 6))
plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (area = {roc_auc:.2f})')
plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Logistic Regression - ROC Curve (Validation Set)')
plt.legend(loc="lower right")

# Evaluate on test set
log_reg_test_pred = best_log_reg.predict(X_test_scaled)
log_reg_test_accuracy = accuracy_score(y_test, log_reg_test_pred)
print(f"\nLogistic Regression test accuracy: {log_reg_test_accuracy:.4f}")

# Print classification report for test set
print("\nTest set classification report:")
print(classification_report(y_test, log_reg_test_pred))

# Create confusion matrix for test set
plt.figure(figsize=(8, 6))
cm = confusion_matrix(y_test, log_reg_test_pred)
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')
plt.title('Logistic Regression - Test Confusion Matrix')
plt.xlabel('Predicted Label')
plt.ylabel('True Label')

# Calculate ROC curve and AUC for test set
log_reg_test_proba = best_log_reg.predict_proba(X_test_scaled)[:, 1]
fpr, tpr, _ = roc_curve(y_test, log_reg_test_proba)
roc_auc = auc(fpr, tpr)

plt.figure(figsize=(8, 6))
plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (area = {roc_auc:.2f})')
plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Logistic Regression - ROC Curve (Test Set)')
plt.legend(loc="lower right")

# Feature importance (coefficients for logistic regression)
plt.figure(figsize=(12, 8))
coef = best_log_reg.coef_[0]
indices = np.argsort(np.abs(coef))[::-1]
plt.title('Logistic Regression Feature Importance (Coefficient Magnitude)')
plt.bar(range(X_train.shape[1]), np.abs(coef[indices]), align='center')
plt.xticks(range(X_train.shape[1]), [feature_names[i] for i in indices], rotation=90)
plt.tight_layout()

# Plot coefficients with direction (positive/negative)
plt.figure(figsize=(12, 8))
plt.title('Logistic Regression Coefficients (with direction)')
plt.bar(range(X_train.shape[1]), coef[indices], align='center')
plt.xticks(range(X_train.shape[1]), [feature_names[i] for i in indices], rotation=90)
plt.tight_layout()

test_probabilities = best_log_reg.predict_proba(X_test_scaled)[:, 1]
plt.figure(figsize=(8, 6))
sns.histplot(test_probabilities, bins=20, kde=True)
plt.title("Histogram of Predicted Probabilities (Test Set)")
plt.xlabel("Disease Probability")
plt.ylabel("Frequency")
plt.tight_layout()
plt.show()

from sklearn.metrics import precision_recall_curve, average_precision_score

precision, recall, thresholds = precision_recall_curve(y_test, test_probabilities)
avg_precision = average_precision_score(y_test, test_probabilities)

plt.figure(figsize=(8, 6))
plt.plot(recall, precision, color='green', lw=2, label=f'PR curve (AP = {avg_precision:.2f})')
plt.xlabel('Recall')
plt.ylabel('Precision')
plt.title('Precision-Recall Curve (Test Set)')
plt.legend()
plt.grid(True)
plt.tight_layout()
plt.show()

thresholds = np.linspace(0, 1, 100)
accuracies = []
precisions = []
recalls = []

for thresh in thresholds:
    preds = (test_probabilities >= thresh).astype(int)
    accuracies.append(accuracy_score(y_test, preds))
    precisions.append(np.mean(preds[y_test == 1]) if np.any(preds) else 0)
    recalls.append(np.mean(preds == y_test))

plt.figure(figsize=(10, 6))
plt.plot(thresholds, accuracies, label='Accuracy')
plt.plot(thresholds, precisions, label='Precision')
plt.plot(thresholds, recalls, label='Recall')
plt.xlabel('Threshold')
plt.ylabel('Score')
plt.title('Metrics vs Decision Threshold')
plt.legend()
plt.grid(True)
plt.tight_layout()
plt.show()

# Save the model
print("\nSaving the Logistic Regression model...")
joblib.dump(best_log_reg, 'logistic_regression_model.pkl')

# Part 2: Test with new data
print("\n\n========= TESTING WITH NEW DATA =========\n")
print("Please upload your Test CSV file...")
uploaded = files.upload()
filename = list(uploaded.keys())[0]
print(f"Test file uploaded: {filename}")

# Load the test dataset
test_df = pd.read_csv(filename)
print("Test dataset loaded successfully!")

# Extract names before dropping any columns
if 'name' in test_df.columns:
    test_names = test_df['name'].values
else:
    test_names = [f"Patient_{i+1}" for i in range(len(test_df))]

# Drop unnecessary columns if present
test_features = test_df.copy()
if 'name' in test_features.columns:
    test_features.drop(columns=['name'], inplace=True)
if 'status' in test_features.columns:
    test_features.drop(columns=['status'], inplace=True)

# Check for feature consistency
print("\nChecking feature consistency...")
missing_cols = set(feature_names) - set(test_features.columns)
extra_cols = set(test_features.columns) - set(feature_names)
if missing_cols:
    print(f"Warning: Test data is missing columns that were in training data: {missing_cols}")
    for col in missing_cols:
        test_features[col] = 0
if extra_cols:
    print(f"Warning: Test data has extra columns that were not in training data: {extra_cols}")
    test_features = test_features.drop(columns=list(extra_cols))

# Ensure columns are in the same order as during training
test_features = test_features[feature_names]

# Scale the test features using the same scaler
test_features_scaled = scaler.transform(test_features)

# Make predictions with Logistic Regression
print("\nMaking predictions with Logistic Regression model...")
test_predictions = best_log_reg.predict(test_features_scaled)
test_probabilities = best_log_reg.predict_proba(test_features_scaled)[:, 1]

# Create results dataframe
results_df = pd.DataFrame({
    'Name': test_names,
    'Predicted_Status': test_predictions,
    'Disease_Probability': test_probabilities
})

# Display results in requested format
print("\nPrediction Results:")
for i in range(len(test_names)):
    prob = test_probabilities[i]
    pred = test_predictions[i]
    print(f"{test_names[i]}: {prob:.4f}")

# Save results to CSV
results_df.to_csv('logistic_regression_predictions.csv', index=False)
print("\nResults saved to 'logistic_regression_predictions.csv'")

# If ground truth is available in test data, evaluate performance
if 'status' in test_df.columns:
    y_true = test_df['status'].astype(int)
    print("\nEvaluation on the new test data:")
    print(f"Accuracy: {accuracy_score(y_true, test_predictions):.4f}")
    print("\nClassification Report:")
    print(classification_report(y_true, test_predictions))

    # Confusion matrix
    plt.figure(figsize=(8, 6))
    cm = confusion_matrix(y_true, test_predictions)
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')
    plt.title('Logistic Regression - New Test Data Confusion Matrix')
    plt.xlabel('Predicted Label')
    plt.ylabel('True Label')
    plt.savefig('log_reg_new_test_confusion_matrix.png')
    plt.close()